---
title: "Homework 8"
author: "Quynh Tran"
date: "March 20, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(partykit)
library (ISLR)
library (ROCR)
library (rpart)
library (tidyverse)
library (broom)
```



###Exercise 1
####Load the train.csv dataset into R. How many observations and columns are there? 891 observations, 12 variables.

```{r}

file_path <- '~/compscix-415-2-assignments/train_titanic.csv'
Train_titanic <- read_csv(file = file_path)
```


###Exercise 2
Our first step is to randomly split the data into train and test datasets. We will use a 70/30 split, and use the random seed of 29283 so that we all should get the same training and test set.

```{r}
Train_titanic <- Train_titanic %>% mutate(Survived_fac = case_when(
  Survived ==  1 ~ '1',
  Survived ==  0 ~ '0'
))

```
```{r}
set.seed(29283) 

# When taking a random sample, it is often useful to set a seed so that your work is reproducible. Setting a seed will guarantee that the same random sample will be generated every time, so long as you always set the # same seed beforehand.

titanic70 <- Train_titanic %>% sample_frac(.70) # Let's create our training set using sample_frac. Fill in the blank.
```

```{r}
titanic30 <- Train_titanic %>% filter(!(Train_titanic$PassengerId %in% titanic70$PassengerId)) 
#Create a testing set from the dataset. Filter observations not in training set in dataset by $Id   
```

###Exercise 3
Our target is called Survived. First, fit a logistic regression model using Pclass, Sex, Fare as your three features. Fit the model using the glm() function.
Ask yourself these questions before fitting the model:
• What kind of relationship will these features have with the probability of survival?
• Are these good features, given the problem we are trying to solve?

####Survived is whether the person survived the Titanic sinking (0=died/no,1=survived/yes); Pclass is the class of passage (1,2,3) which approximates social class and ability to pay for a higher fare; Sex is male/female; and Fare is a numeric value. Fare and Pclass would be highly correlated features. The higher the fare, the more likely it would be a first class ticket. Both would be related to survival. First class passengers' rooms are on the upper level and during the sinking of the Titanic, it would have been easier for them to escape. Lower class passengers' rooms are in the lower decks and harder to exit to survive. Also during the evacuation, they would evacuate first class passengers before anyone else. 

####Sex is a good feature. They would likely evacuate women before men, although men would be more fit to survive once evacuated. Age would be a good feature to consider. Younger passengers should be fitter for survival, although they probably evacuate older passengers and children first.


After fitting the model, output the coefficients using the broom package and answer these questions:
• How would you interpret the coefficients? • Are the features significant?

####All the features in the model are significant. Sex has the strongest effect, being male decreases the log-odds of survival, 2.8 times more likely to die than if the passenger were female.

####Passenger class also have an effect. The higher the class, the more likely the passenger would die, that is first class passenger are least likely to die whereas 3rd class passnger have the highest probability of death. The log odds of 0.87 times more likely to die as Pclass increases from 1st to 3rd class. 

####While Fare is statistically significant, the higher the fare, the more likely the person survived, but the coefficient is very small (0.0018) and therefore its effect is small. I would not use this in the model. I expect it's highly correlated with Pclass and most of the effect on survival can be explained by Pclass already. 


```{r}
survival <- glm(Survived ~ Pclass + Sex + Fare, data = titanic70, family = 'binomial')
# take a look at the features and coefficients
tidy(survival)

```


###Exercise 4
Now, let’s fit a model using a classification tree, using the same features and plot the final decision tree. Use the code below for help.
Answer these questions:
• Describe in words one path a Titanic passenger might take down the tree. (Hint: look at your tree, choose a path from the top to a terminal node, and describe the path like this - a male passenger who paid a fare > 30 and was in first class has a high probability of survival)

####A female passenger in 1st or 2nd class has the highest probability of survival. A female in 3rd class whose fare is between $7.88 and $15 has the second best probability of survival. A female passenger in 3rd class who paid less than $7.88 has the third best probability of survival.

• Does anything surprise you about the fitted tree?

####I was surprised how strong sex was a factor in survival. 

```{r}

library(rpart) 
library(partykit)
tree_mod <- rpart(Survived_fac ~ Pclass + Sex + Fare, data = titanic70) 
plot(as.party(tree_mod))
```

Exercise 5
Evaluate both the logistic regression model and classification tree on the test_set. First, use the predict() function to get the model predictions for the testing set. Use the code below for help.
It may seem odd that we are using the same predict() function to make predictions for two completely different types of models (logistic regression and classification tree). This is a feature of R that there are many generic functions that you can apply to different R objects. Depending on the class of the object that is passed to the generic function, R will know which definition of the generic function to use on that object.

```{r}
test_logit <- predict(survival, newdata = titanic30, type = 'response') 

test_tree <- predict(tree_mod, newdata = titanic30)[,2]
```

(a) Next, we will plot the ROC curves from both models using the code below. Don’t just copy and paste the code. Go through it line by line and see what it is doing.

```{r}
 library(ROCR)
# create the prediction objects for both models
pred_logit <- prediction(predictions = test_logit, labels = titanic30$Survived) 
pred_tree <- prediction(predictions = test_tree, labels = titanic30$Survived)
# get the FPR and TPR for the logistic model
# recall that the ROC curve plots the FPR on the x-axis
perf_logit <- performance(pred_logit, measure = 'tpr', x.measure = 'fpr') 
perf_logit_tbl <- tibble(perf_logit@x.values[[1]], perf_logit@y.values[[1]])
# Change the names of the columns of the tibble
names(perf_logit_tbl) <- c('fpr', 'tpr')
# get the FPR and TPR for the tree model
perf_tree <- performance(pred_tree, measure = 'tpr', x.measure = 'fpr') 
perf_tree_tbl <- tibble(perf_tree@x.values[[1]], perf_tree@y.values[[1]])
# Change the names of the columns of the tibble
names(perf_tree_tbl) <- c('fpr', 'tpr')
# Plotting function for plotting a nice ROC curve using ggplot
plot_roc <- function(perf_tbl) {
p <- ggplot(data = perf_tbl, aes(x = fpr, y = tpr)) + geom_line(color = 'blue') +
geom_abline(intercept = 0, slope = 1, lty = 3) +
labs(x = 'False positive rate', y = 'True positive rate') + theme_bw()
return(p)
}
# Create the ROC curves using the function we created above
plot_roc(perf_logit_tbl) 
plot_roc(perf_tree_tbl)
```


(b) Now, use the performance() function to calculate the area under the curve (AUC) for both ROC curves. Check ?performance for help on plugging in the right measure argument.

```{r}
 # calculate the AUC
auc_logit <- performance(pred_logit, measure = "auc") 
auc_tree <- performance(pred_tree, measure = "auc")
# extract the AUC value
auc_logit@y.values[[1]] 
auc_tree@y.values[[1]]
```

What do you notice about the ROC curves and the AUC values? Are the models performing well? Is the logistic regression model doing better, worse, or about the same as the classification tree?

####Visually it's a harder the differentiate the ROC curves of the logistic regression and classification tree model. The logistic curve is fatter and has a rounder curve whereas the tree model curve has a flatter curve.

####The area under the curve (AUC) of the logistic regression model is closer to 1, at 0.81 than the AUC of the classification model, at 0.77. The AUC values show that the logistic regression model is better.


(c) Lastly, pick a probability cutoff by looking at the ROC curves. You pick, there’s no right answer (but there is a wrong answer - make sure to pick something between 0 and 1). 

Using that probability cutoff, create the confusion matrix for each model by following these steps:

1. Pick a cutoff value.

2. Append the predicted probability values from each model (you created these at the beginning of
Exercise 5) to your test_set tibble using mutate().

3. Create a new column for the predicted class from each model using mutate() and case_when(). Your new predicted class columns can have two possible values: yes or no which represents whether or not the passenger is predicted to have survived or not given the predicted probability.

4. You should now have 4 extra columns added to your test_set tibble, two columns of predicted probabilities, and two columns of the predicted categories based on your probability cutoff.
5. Now create the table using the code below:

titanic30 %>% count(_____, Survived) %>% spread(Survived, n) 
titanic30 %>% count(_____, Survived) %>% spread(Survived, n)

```{r}

test_logit <- predict(survival, newdata = titanic30, type = 'response') 
head (test_logit)

titanic30 <- titanic30 %>% mutate(preds_prob = test_logit) %>% 
  mutate(preds_cat = case_when(preds_prob < .25 ~ 'No',
                               preds_prob >= .25 ~ 'Yes'))

titanic30 %>% count(preds_cat)

titanic30 %>% count(preds_cat, Survived) %>% spread(Survived, n)

```

```{r}

test_tree <- predict(tree_mod, newdata = titanic30)
head(test_tree)

titanic30 <- titanic30 %>% mutate(preds_prob_tree = test_tree[, 2]) %>% 
  mutate(preds_cat_tree = case_when(preds_prob_tree < .25 ~ 'No',
                               preds_prob_tree >= .25 ~ 'Yes'))

titanic30 %>% count(preds_cat_tree)

titanic30 %>% count(preds_cat_tree, Survived) %>% spread(Survived, n)
```

If you choose a cutoff of 0.25, your confusion tables should look like this:

   class_tree
 * <chr>
 1 No           120    22
 2 Yes           42    83


###Exercise 6 (OPTIONAL - won’t be graded)
Feel free to play around with logistic regression and classification trees. Add some other features and see how the model results change. Test the model on test_set to compare the ROC curves. Bonus points (still won’t be graded) for plotting all ROC curves on the same plot.


```{r}
survival2 <- glm(Survived ~ Pclass + Sex + Age, data = titanic70, family = 'binomial')
# take a look at the features and coefficients
tidy(survival2)

survival3 <- glm(Survived ~ Pclass + Sex + Fare + Age, data = titanic70, family = 'binomial')
# take a look at the features and coefficients
tidy(survival3)
```

```{r}
tree_mod2 <- rpart(Survived_fac ~ Pclass + Sex + Fare, data = titanic70) 
plot(as.party(tree_mod2))

tree_mod3 <- rpart(Survived_fac ~ Pclass + Sex + Fare+ Age, data = titanic70) 
plot(as.party(tree_mod3))
```

