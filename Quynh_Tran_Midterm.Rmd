---
title: "COMPSCIX 415.2 Homework 5/Midterm"
author: "Quynh Tran"
date: "February 27, 2018"
output: 
  html_document: 
    toc: yes
    toc_depth: 2
---
# https://github.com/mightyquynh/compscix-415-2-assignments

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


#The tidyverse packages (3 points)


```{r}
library(tidyverse)
sessionInfo()

```

##1) Can you name which package is associated with each task below? 
###Plotting: ggplot2
###Data munging/wrangling - dplyr
###Reshaping (speading and gathering) data - dplyr
###Importing/exporting data - readr

## 2) Now can you name two functions that you’ve used from each package that you listed above for these tasks?
###Plotting - geom_boxplot() geom_line()
###Data munging/wrangling - filter() select()
###Reshaping data - spread() gather()
###Importing/exporting data (note that readRDS and saveRDS are base R functions) - read_delim() write_csvc()


#R Basics (1.5 points)
## 1) Fix this code with the fewest number of changes possible so it works. My_data.name___is.too0ooLong! <- c( 1 , 2   , 3 ) Just removing ! will make it work, although the data name is very bad:


```{r}


My_data.name___is.too0ooLong <- c( 1 , 2   , 3 )

My_data.name___is.too0ooLong

```

## 2) Fix this code so it works. c has to be small cap, and 'it' was missing a closing '.

```{r}
y_string <- c('has', 'an', 'error', 'in', 'it')

y_string

```

## 3) Look at the code below and comment on what happened to the values in the vector. A vector must have elements of the same type. While 1, 2, 5 were entered as numerics, and '3' and '4' as characters, it made all of them characters. 

```{r}
my_vector <- c(1, 2, '3', '4', 5)
my_vector

typeof(my_vector)
```



#Data import/export 

```{r}

file_path <- '~/compscix-415-2-assignments/rail_trail.txt'
rail_trail <- read_delim(delim = '|', file = file_path)

glimpse(rail_trail)
```


```{r}
saveRDS(rail_trail, file = "~/compscix-415-2-assignments/rail_trail.rds")

rail_trail<- readRDS(file ='~/compscix-415-2-assignments/rail_trail.rds')

glimpse(rail_trail)
```


# Visualization
##1) The Mrs. President pie charts would be better as bar charts. Whole colored pies represent 100% but the yes/no values are separated as separate pies, rather than parts of the pie to represent 100%. Even if the yes/no values were represented within one pie, one problem is that the yes/no sum don't total 100%, so it would be misleading still. It should also be 2 separate graphics, age is a different variable from gender. Even though the gender pie is a different color that the age pies, it's lined up as a group.

 
```{r}

library(forcats)
president <- tribble(
  ~age,     ~Agree, ~value, 
  "under45",  "yes",  79, 
   "under45", "no",   16,
  "b45-64",   "yes", 69, 
  "b45-64",  "no",   22,
  "65over", "yes", 44,
  "65over", "no", 39,
)

president2 <-tribble(
  ~gender, ~Agree, ~value,
  "Men", "yes", 65,
  "Men",  "no", 25,
  "Women", "yes",72,
  "Women", "no", 20,
)
president <- president %>% mutate(age = fct_relevel(age, 'under45', 'b45-64', '65over'))
president


ggplot(data=president)+
  geom_bar(mapping=aes(x=age, y=value, fill=Agree), position="dodge", stat="identity") + 
labs(x="Age group", y="Percentage", title="% respondents who agree a woman will be president in their lifetime")

ggplot(data=president2)+
  geom_bar(mapping=aes(x=gender, y=value, fill=Agree), position="dodge", stat="identity") + 
labs(x="gender", y="Percentage", title="% respondents who agree a woman will be president in their lifetime")

```


## 2) Reproduce diamonds graphics

## 3) Make one change on the code and make graphics more useful. More useful would be to sort by the median size of diamond carat by cut. With this graph, I can tell that the fair cut has the largest median size of diamonds. 

```{r}
library(tidyverse)

ggplot(data = diamonds, mapping = aes(x = cut, y = carat, fill=color))+
  geom_boxplot(position='identity')+ 
  coord_flip() +
  xlab("CUT OF DIAMOND")+
  ylab("CARAT OF DIAMOND")

ggplot(data = diamonds, mapping = aes(x = reorder(cut, carat, FUN=median), y = carat, fill=color))+
  geom_boxplot(position='identity')+ 
  coord_flip() +
  xlab("CUT OF DIAMOND")+
  ylab("CARAT OF DIAMOND") 
  
ggplot(data = diamonds, mapping = aes(x = reorder(cut, carat), y = carat, fill=color))+
  geom_boxplot(position='identity')+ 
  coord_flip() +
  xlab("CUT OF DIAMOND")+
  ylab("CARAT OF DIAMOND") 



```

# Data munging and wrangling 
## 1) Tidy table2, by spreading, it pulled the "type" of cases and populations into new variable.

```{r}
table2

table2 %>%
spread(key=type, value=count)
```

## 2) diamonds with price per carat
### mutate(diamonds, price_per_carat = price/carat)
### diamonds


## 3) Number of diamonds price > 1000 and carat < 1.5 by cut. Ideal	13044, Premium	8348, Very Good	7462, Good	3153, Fair	1174	

```{r}
library(dplyr)
library(tidyverse)

diamonds %>%
  group_by (cut) %>%
  count(cut, sort = TRUE)

diamonds %>%
filter(price > 1000 & carat < 1.5) %>%
  group_by (cut) %>%
  count(cut, sort = TRUE)

```
### What proportion of diamonds price > 1000 and carat < 1.5 by cut? Ideal 0.6052619, Premium	0.6053223,	Very Good	0.6176130,	Good	0.6426824,	Fair	0.7291925. First I plot the distribution of diamonds by price and carat by cut to explore the distribution. Regardless of the cut, there's a positive relationship between price and carat. The plots are well distributed. Diamonds that are more than $1000 but smaller than 1.5 carat would fall in the left bottom of the plot, ones that are relatively expensive but relatively small. Compared to all the plotted diamonds, it seems improbable that such a large proportion of these diamonds are in the data set. Something is wrong with this data. 	


```{r}

ggplot(data = diamonds) + 
  geom_point(mapping = aes(x = carat, y = price)) +
facet_wrap(~ cut)


prop_diamonds <- tribble(
  ~cut, ~total, ~subtotal,

"Ideal",21551	,	13044,			
"Premium",13791	,	8348,			
"Very Good",12082	,	7462,			
"Good",	4906	,3153,			
"Fair", 1610,	1174,	
)

prop_diamonds

mutate(prop_diamonds, proportion = subtotal/total)


```





# EDA txhousing data

## 1) During what time period is this data from? 2000-2015

```{r}
glimpse(txhousing)

```


## 2) How many cities are represented? 46 cities

```{r}
txhousing %>% 
  group_by(city) %>% 
  summarise(
    count = n())

txhousing %>% 
  count(city)
```


## 3) Which city, month and year had the highest number of sales? Houston, July 2015 had 8945 sales.

```{r}

txhousing %>% 
  group_by(city, month, year) %>% 
  summarise(sales = mean(sales, na.rm = TRUE)) %>%
 arrange(desc(sales))
```

## 4) What kind of relationship do you think exists between the number of listings and the number of sales? There is a positive relationship between listings and sales. The relationship holds true even when you remove or don't remove missing variables. This relationship holds true for the 15-year period. The relationship is not clear when looking at different cities though.

```{r}
ggplot(data = txhousing) + 
  geom_point(mapping = aes(x = listings, y = sales)) +
  geom_smooth(mapping = aes(x = listings, y = sales))

ggplot(data = txhousing) + 
  geom_point(mapping = aes(x = listings, y = sales), na.rm=FALSE) +
  geom_smooth(mapping = aes(x = listings, y = sales), na.rm=FALSE)

ggplot(data = txhousing) + 
  geom_point(mapping = aes(x = listings, y = sales), na.rm=TRUE) +
  geom_smooth(mapping = aes(x = listings, y = sales), na.rm=TRUE)

ggplot(data = txhousing) + 
  geom_point(mapping = aes(x = listings, y = sales)) +
facet_wrap(~ year, nrow = 3)

ggplot(data = txhousing) + 
  geom_point(mapping = aes(x = listings, y = sales)) +
facet_wrap(~ city)


```

Check your assumption and show your work.
## 5) What proportion of sales is missing for each city?

```{r}
sales_missing <- txhousing %>% 
  filter(is.na(sales))

txhousing%>% group_by(city) %>% summarize(sales_city = n())

sales_missing %>% group_by(city) %>% summarize(missing_count = n())
```




## 6 Looking at only the cities and months with greater than 500 sales:

```{r}
sales_m500 <- txhousing %>% 
  filter(sales > 500)
```

###• Are the distributions of the median sales price (column name median), when grouped by city, different? The same? Show your work. Compare the distribution of median between cities, don't need sales in the plot. Yes, the ditributions differ between cities. A city like Corpus Cristi has a much narrower distribution of median sales.  

```{r}
sales_m500 <- txhousing %>% 
  filter(sales > 500)

ggplot(data = sales_m500) + 
  geom_boxplot(mapping = aes(x = median, y=city), position="identity")

```


```{r}
txhousing %>% filter(sales > 500) %>%
  group_by(city) %>%
  summarise(prop_median = mean(median, na.rm = TRUE)) %>%
  ggplot(aes(y = prop_median, x = city)) +
  geom_bar(stat = 'identity')+ coord_flip()
  
```






```{r}

sales_m500 <- txhousing %>% 
  filter(sales > 500)

ggplot(data = sales_m500) + 
  geom_point(mapping = aes(x = sales, y = median))+
  geom_smooth(mapping = aes(x = sales, y = median)) +
facet_wrap(~ city)


```


  
###6) I think I'm over-thinking this question but this is how I would explore median sales across cities. Cities like Houston and Dallas have sales clustered greater than 2500, although the median price range is fairly small.  That is the slope of these lines are pretty flat (small). Cities like Fort Bend, Montgomery County, Denton County and the Bay Area have much steeper slopes, there's fewer sales for the median price of sales. 

### Any cities that stand out that you’d want to investigate further? I'd want to investigate  Dallas and Houston further first. Then Austin, Collin County, and San Antonio as another batch. Arlington, Bay Area, Corpus Christi, Denton County, Fort Bend, El Paso, Fort Worth, Montgomery County, NE Tarrant County as another batch. They seem to break out into 3 groups with high, medium, and low amount of sales.

```{r}
sales_m500 <- txhousing %>% 
  filter(sales > 500)

  ggplot(data = sales_m500) + 
  geom_point(mapping = aes(x = sales, y = median)) +
facet_wrap(~ city)
```


###• Why might we want to filter out all cities and months with sales less than 500? When I plot all cities' sales number vs median sales price, you see a steep curve at the lower end of sales range. So let's explore the distribution sales greater than 500 and less than 500.  Now there's still a bump which makes me want to explore the relationship for sales less than 1000 and greater than 1000. These results might be related to the size of the cities. Larger cities have higher sales, smaller cities have less sales, which some of the earlier plots point to. You might also want the analysis to remove sales less than 500 to remove the smaller cities, or winter months (sales tend to be less frequent then).

```{r}

ggplot(data = txhousing) + 
  geom_point(mapping = aes(x = sales, y = median)) +
  geom_smooth(mapping = aes(x = sales, y = median))


sales_gt500 <- txhousing %>% 
  filter(sales >= 500)

ggplot(data = sales_gt500) + 
  geom_point(mapping = aes(x = sales, y = median)) +
  geom_smooth(mapping = aes(x = sales, y = median))


sales_LT500 <- txhousing %>% 
  filter(sales < 500)

ggplot(data = sales_LT500) + 
  geom_point(mapping = aes(x = sales, y = median)) +
  geom_smooth(mapping = aes(x = sales, y = median))


sales_gt1000 <- txhousing %>% 
  filter(sales >= 1000)

ggplot(data = sales_gt1000) + 
  geom_point(mapping = aes(x = sales, y = median)) +
  geom_smooth(mapping = aes(x = sales, y = median))

sales_LT1000 <- txhousing %>% 
  filter(sales < 1000)

ggplot(data = sales_LT1000) + 
  geom_point(mapping = aes(x = sales, y = median)) +
  geom_smooth(mapping = aes(x = sales, y = median))


  ggplot(data = sales_gt500) + 
  geom_point(mapping = aes(x = sales, y = median)) +
facet_wrap(~ city)
  
  
  ggplot(data = sales_LT500) + 
  geom_point(mapping = aes(x = sales, y = median)) +
facet_wrap(~ city)
```

